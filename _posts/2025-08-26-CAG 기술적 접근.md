---
layout: post
title: CAG 자세히 살펴보기
subtitle: CAG의 WHY와 HOW의 기술적 접근
author: Garpas
categories: LLM
tags:
  - LLM
  - "#CAG"
  - why-how
excerpt_image: /assets/images/thumbnails/mayling_l.png
hidden: "true"
---
이 포스트에서는 **CAG(CACHE AUGMENTED GENERATION)** 에대하여 기술적으로 접근합니다.

# 0. Prior Knowledge
- RAG
- LLM
- cache memory
# 1. Background & WHY
RAG의 단점은
1. retrival latency
2. potential errors in document slection

CAG는 knowledge를 미리 preloading해 속도를 획기적으로 줄인 방법
![RAG CAG 구조](/assets/images/post_image/RAG-CAG-Structure.png)
![RAG CAG 프롬프트](/assets/images/post_image/RAG-CAG-prompt.png)
# 2. HOW
1. key value로 어떻게 나타내는 방법
2. query가 들어왔을 때 어떻게 relevent한 정보를 찾음?
	transformer구조를 사용하여 key와 query를 비교하여 비슷한 key의 value가 더 크게 반영
3. KnowledgeBase 전체를 prompt에 저장하지만 고정된 정보이기 때문에 openAI api의 경우 50% 할인 혜택이 있음
openAPI 기준 5~10분 안에 같은 토큰을 사용할 경우 자동으로 cache 가격으로 적용
   
| 항목                    | RAG                    | CAG                  |
| --------------------- | ---------------------- | -------------------- |
| **API 첫 토큰 응답 시간**    | **1430 ms**            | **1019 ms**          |
| **Retriever 소요 시간**   | 674 ms                 | 0 ms                 |
| **LLM 응답 생성 시간**      | 712 ms                 | 809 ms               |
| **평균 비용 (OpenAI 기준)** | **$0.01258 (약 17.7원)** | **$0.01205 (약 17원)** |

  
50개 정도의 문서에서는 높은 정확도를 얻었지만 500개 이상 되는 문서에서는 오히려 성능 저하
# 3. Use Cases
# 4. Source
https://papooo-dev.github.io/posts/cag-openai/ㄴ
