---
layout: post
title: RAG 자세히 살펴보기
subtitle: RAG의 WHY와 HOW의 기술적 접근
author: Garpas
categories: LLM
tags:
  - LLM
  - RAG
  - why-how
excerpt_image: /assets/images/thumbnails/mayling_l.png
---
이 포스트에서는 **RAG(Retrival-Augmented Generation, 검색증강생성)** 에 대하여 깊게 기술적으로 살펴보고자 합니다. 기본적인 소개나 논문 리뷰보다는 왜 만들어 졌는지(WHY), 어떻게 작동하는지(HOW)를 기술적으로 접근하고자 합니다.

# 0.Prior Knowledge
- LLM (작성 예정)
- VectorDB (작성 예정)

논문 [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
Facebook AI Research, Accepted at NeurIPS 2020
# 1. Background & WHY
RAG가 제안되기 전(~2019)의 LLM은 Seq2Seq 모델이 주류였습니다. 처음에는 LSTM이나 GRU등을 사용하였고 이후에는 Attention과 Transformer등을 활용하는 모델이 등장하였습니다. 대규모 사전학습으로 문맥 이해 능력이 획기적으로 강화된 BERT, GPT, T5, BART가 LLM의 대표 주자였습니다. 하지만 모델이 알고 있는 사실은 학습 시점의 데이터에 한정되었고 새로운 정보나 특정 도메인의 지식을 추가하려면 학습을 요구하였습니다.
이를 해결하기 위해 DrQA와 같이 검색 시스템으로 문서를 찾고, 문서 안에서 답을 추출하는 방법 등이 새롭게 제안되고 있었습니다.

논문에서 제시한 LLM의 문제점은 크게 세가지 입니다.
1. They cannot easily expand or revise their memory. 메모리를 확장하거나 수정하기 힘듭니다.
2. Can't straightforwardly provied insight into their predictions. 왜 정답을 도출하였는지 알려주지 않습니다.
3. May produce hallucinations. 할루시네이션을 만들 수 있습니다.

RAG는 사전 학습된 seq2seq model과 벡터로 이루어진 non-parametric memory를 결합하는 방법을 제안하여 위 문제를 해결하고자 하였습니다.
# 2.HOW
논문에서 제안하는 방법
![RAG 모델 구조](/assets/images/post_image/rag-overall-structure.png)
> Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi . For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.

Input Sequence x -> text documents z -> target sequence y 순으로 사용됩니다. RAG는 크게 두 가지 모델로 Retriever와 Generator 이루어져 있습니다.
- **Retrieval:** The system searches trusted sources—such as databases, research papers, or enterprise knowledge bases—for information relevant to a user’s query.
- **Generation:** A language model synthesizes this retrieved data into a clear, accurate, and contextual response.
Retrieval로 입력된 쿼리에 유사한 Document 구문을 가져오고 Generation을 통해 정확한 정답을 만들어냅니다.
RAG의 핵심은 외부에서 지식을 가져오는 Retrieval 시스템이기 때문에 Retriever 시스템을 집중적으로 파악하겠습니다.

### 데이터 임베딩
RAG를 제대로 활용하기 위해서는 외부 정보를 가져올 수 있도록 정리가 되어있어야 합니다.
일반적으로 벡터 임베딩을 통해 검색하고 비교하기 쉽게 만듭니다. 벡터로 임베딩하는 과정은 다음과 같습니다.   
(그림)
문서 (텍스트)   
↓ 청킹/토크나이징   
토큰 시퀀스 (예: 30개)   
↓ Transformer 인코딩   
토큰별 벡터 (30, 768)   
↓ 풀링, 정규화   
최종 임베딩 (768, norm=1)   
↓ 벡터DB 저장   

데이터 수집·청킹·벡터화·메타데이터 부여 등 **RAG 검색 프로세스**가 성능을 좌우   
1. 청킹: LangChain, LlamaIndex, Semantic Text Splitter같은 도구를 사용
	한 청크의 크기는 기본적으로 1000자 overlap 200, 커스텀 가능
2. 벡터화:
	- OpenAI `text-embedding-3-large` / `text-embedding-3-small`
	- HuggingFace 모델 (예: `sentence-transformers/all-MiniLM-L6-v2`)
	- 한국어 특화 모델 (예: KoSimCSE, KULLM 임베딩 등)   
	한국어 모델은 768 크기의 벡터면 충분   
	토큰 개수만큼 생성됨
3. 풀링, 정규화
	벡터화는 토큰의 개수만큼 생성됨


{
  "id": "chunk_00023",
  "vector": [0.0123, -0.8432, 0.2211, ... 768차원 ...],
  "metadata": {
    "doc_id": "doc_001",
    "chunk_index": 23,
    "source": "medical_paper_2024.pdf",
    "page": 12,
    "text": "Pooling은 토큰별 임베딩을 문서 전체 임베딩으로 요약하기 위해 사용된다."
  }
}

LangChain으로 구현의 큰부분이 이미 정리되어있음
- 문서 로드 → 텍스트 청킹 → 임베딩 생성 → 벡터DB에 저장
- 쿼리 입력 시에도 **쿼리를 같은 임베딩 모델로 벡터화**해서 검색

### 데이터 검색
DB의 크기에 따라 수백만개의 벡터가 생성됨
일반적으로 Dense Retrieval로 순수한 벡터 검색
Sparse + Dense (하이브리드 검색) BM25 같은 키워드 검색 + 벡터 검색을 조합.
Re-ranking ANN으로 후보군 top 50을 추리고 거기서 더 정교한 top5을 검색

# 3.Use cases
신한투자증권&스켈터랩스
약 15000건 문서를 검색 https://www.skelterlabs.com/blog/rag-securities


# 4.Source
