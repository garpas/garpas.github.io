---
layout: post
title: RAG 자세히 살펴보기
subtitle: RAG의 WHY와 HOW의 기술적 접근
author: Garpas
categories: LLM
tags:
  - LLM
  - RAG
  - why-how
excerpt_image: /assets/images/thumbnails/mayling_l.png
---
**"우리 망해써여 지히간니이임!! 엘모호 완전 구멍 송송 바람 숭숭이에여어!!"**

이 포스트에서는 **RAG(Retrival-Augmented Generation, 검색증강생성)** 에 대하여 깊게 기술적으로 살펴보고자 합니다. 기본적인 소개나 논문 리뷰보다는 왜 만들어 졌는지(WHY), 어떻게 작동하는지(HOW)를 기술적으로 접근하고자 합니다.

# 0. Prior Knowledge
- LLM (작성 예정)
- VectorDB (작성 예정)

논문 [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
Facebook AI Research, Accepted at NeurIPS 2020
# 1. Background & WHY
RAG가 제안되기 전(~2019)의 LLM은 Seq2Seq 모델이 주류였습니다. 처음에는 LSTM이나 GRU등을 사용하였고 이후에는 Attention과 Transformer등을 활용하는 모델이 등장하였습니다. 대규모 사전학습으로 문맥 이해 능력이 획기적으로 강화된 BERT, GPT, T5, BART가 LLM의 대표 주자였습니다. 하지만 모델이 알고 있는 사실은 학습 시점의 데이터에 한정되었고 새로운 정보나 특정 도메인의 지식을 추가하려면 학습을 요구하였습니다.
이를 해결하기 위해 DrQA와 같이 검색 시스템으로 문서를 찾고, 문서 안에서 답을 추출하는 방법 등이 새롭게 제안되고 있었습니다.

논문에서 제시한 LLM의 문제점은 크게 세가지 입니다.
1. They cannot easily expand or revise their memory. 메모리를 확장하거나 수정하기 힘듭니다.
2. Can't straightforwardly provied insight into their predictions. 왜 정답을 도출하였는지 알려주지 않습니다.
3. May produce hallucinations. 할루시네이션을 만들 수 있습니다.

RAG는 사전 학습된 seq2seq model과 벡터로 이루어진 non-parametric memory를 결합하는 방법을 제안하여 위 문제를 해결하고자 하였습니다.
# 2. HOW
논문에서 제안하는 방법
![RAG 모델 구조](/assets/images/post_image/rag-overall-structure.png)
>Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to find the top-K documents zi . For final prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.

Input Sequence x -> text documents z -> target sequence y 순으로 사용됩니다. RAG는 크게 두 가지 모델로 Retriever와 Generator 이루어져 있습니다.
- **Retrieval:** The system searches trusted sources—such as databases, research papers, or enterprise knowledge bases—for information relevant to a user’s query.
- **Generation:** A language model synthesizes this retrieved data into a clear, accurate, and contextual response.
# 3. Use cases

# 4. Source